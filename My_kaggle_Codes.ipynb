{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pycaret for finding models"
      ],
      "metadata": {
        "id": "vaN-Cm4LX_vy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycaret[full]\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pycaret.regression import *\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "Ed7dMqreYK3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oswzr_7mp4s4"
      },
      "outputs": [],
      "source": [
        "file_path1 = \"/content/train.csv\"\n",
        "file_path2 = \"/content/test.csv\"\n",
        "data1 = pd.read_csv(file_path1)\n",
        "data2 = pd.read_csv(file_path2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finding models based on R2 score"
      ],
      "metadata": {
        "id": "41QBlF5aYSiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pycaret.regression import *\n",
        "# Read the data\n",
        "train = pd.read_csv('/content/train.csv')\n",
        "test = pd.read_csv('/content/test.csv')\n",
        "# Initialize PyCaret setup\n",
        "reg = setup(\n",
        " data=train,\n",
        " target='target',\n",
        " session_id=123,\n",
        " verbose=False\n",
        ")\n",
        "# Compare models and store results\n",
        "models = compare_models(n_select=15) # Compare top 15 models\n",
        "# Get the comparison results as a DataFrame\n",
        "comparison_df = pull()\n",
        "# Create predictions for each model\n",
        "predictions = {}\n",
        "for i, model in enumerate(models, 1):\n",
        "# Train the model\n",
        " final_model = create_model(model)\n",
        "# Make predictions on test set\n",
        " pred = predict_model(finalmodel, data=test)\n",
        " predictions[f'Model{i}_{type(model).name}'] = pred['prediction'].values\n",
        "# Create submission DataFrame with predictions from all models\n",
        "submissions = pd.DataFrame({'id': test['id']})\n",
        "for model_name, preds in predictions.items():\n",
        " submissions[model_name] = preds\n",
        "# Calculate R² scores on training data for each model\n",
        "print(\"\\nModel R² Scores on Training Data:\")\n",
        "print(comparison_df[['Model', 'R2']].to_string())\n",
        "# Save predictions to CSV\n",
        "submissions.to_csv('model_predictions.csv', index=False)\n",
        "# Get the best model based on R²\n",
        "best_model_name = comparison_df.iloc[0]['Model']\n",
        "best_model_r2 = comparison_df.iloc[0]['R2']\n",
        "print(f\"\\nBest Model: {best_model_name}\")\n",
        "print(f\"Best R² Score: {best_r2:.4f}\")\n",
        "# Show first few predictions from different models\n",
        "print(\"\\nSample Predictions from Different Models:\")\n",
        "print(submissions.head())"
      ],
      "metadata": {
        "id": "_RlIafa5YRZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pycaret.regression import *\n",
        "\n",
        "# Read the data\n",
        "train = pd.read_csv('/content/train.csv')\n",
        "test = pd.read_csv('/content/test.csv')\n",
        "\n",
        "# Create a copy of test data without the id column for feature consistency\n",
        "test_features = test.drop('id', axis=1)\n",
        "\n",
        "# Merge train and test data\n",
        "all_data = pd.concat([train, test_features], axis=0, ignore_index=True)\n",
        "\n",
        "# Remove rows with missing values in the target column\n",
        "all_data = all_data.dropna(subset=['target'])\n",
        "\n",
        "# Initialize PyCaret setup with merged data\n",
        "reg = setup(\n",
        "    data=all_data,\n",
        "    target='target',\n",
        "    train_size=0.8,\n",
        "    session_id=123,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Compare models and store results\n",
        "models = compare_models(n_select=15)  # Compare top 15 models\n",
        "\n",
        "# Get the comparison results as a DataFrame\n",
        "comparison_df = pull()\n",
        "\n",
        "# Create predictions for each model\n",
        "predictions = {}\n",
        "for i, model in enumerate(models, 1):\n",
        "    # Train the model\n",
        "    final_model = create_model(model)\n",
        "\n",
        "    # Make predictions on test set\n",
        "    pred = predict_model(final_model, data=test)\n",
        "    predictions[f'Model_{i}_{type(model).__name__}'] = pred['prediction'].values\n",
        "\n",
        "# Create submission DataFrame with predictions from all models\n",
        "submissions = pd.DataFrame({'id': test['id']})\n",
        "for model_name, preds in predictions.items():\n",
        "    submissions[model_name] = preds\n",
        "\n",
        "# Calculate R² scores on training data for each model\n",
        "print(\"\\nModel R² Scores on Training Data:\")\n",
        "print(comparison_df[['Model', 'R2']].to_string())\n",
        "\n",
        "# Save predictions to CSV\n",
        "submissions.to_csv('model_predictions.csv', index=False)\n",
        "\n",
        "# Get the best model based on R²\n",
        "best_model_name = comparison_df.iloc[0]['Model']\n",
        "best_model_r2 = comparison_df.iloc[0]['R2']\n",
        "\n",
        "print(f\"\\nBest Model: {best_model_name}\")\n",
        "print(f\"Best R² Score: {best_r2:.4f}\")\n",
        "\n",
        "# Show first few predictions from different models\n",
        "print(\"\\nSample Predictions from Different Models:\")\n",
        "print(submissions.head())\n",
        "\n",
        "# Create a final submission with the best model's predictions\n",
        "best_predictions = submissions[['id', f'Model_1_{type(models[0]).__name__}']]\n",
        "best_predictions.columns = ['id', 'target']\n",
        "best_predictions.to_csv('best_model_predictions.csv', index=False)\n",
        "\n",
        "# Print diagnostic information\n",
        "print(\"\\nShape of merged data before cleaning:\", len(train) + len(test))\n",
        "print(\"Shape of merged data after cleaning:\", len(all_data))\n",
        "print(\"\\nMissing values in cleaned merged data:\")\n",
        "print(all_data.isnull().sum())"
      ],
      "metadata": {
        "id": "Y2TwfsN1Ym01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model based on MAPE"
      ],
      "metadata": {
        "id": "oW2hbnCgYnon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pycaret.regression import *\n",
        "\n",
        "def prepare_features(df):\n",
        "    \"\"\"\n",
        "    Prepare time series features from the date column and clean the data\n",
        "    \"\"\"\n",
        "    # Convert date to datetime\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "    # Extract date features\n",
        "    df['year'] = df['date'].dt.year\n",
        "    df['month'] = df['date'].dt.month\n",
        "    df['day_of_week'] = df['date'].dt.dayofweek\n",
        "    df['day_of_month'] = df['date'].dt.day\n",
        "\n",
        "    # Create categorical features\n",
        "    df['store_product'] = df['store'] + \"_\" + df['product']\n",
        "\n",
        "    return df\n",
        "\n",
        "# Read the data\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "# Remove rows with missing target values from training data\n",
        "print(f\"Training rows before cleaning: {len(train)}\")\n",
        "train = train.dropna(subset=['num_sold'])\n",
        "print(f\"Training rows after cleaning: {len(train)}\")\n",
        "\n",
        "# Prepare features for both train and test\n",
        "train = prepare_features(train)\n",
        "test = prepare_features(test)\n",
        "\n",
        "# Sort data by date for time series modeling\n",
        "train = train.sort_values('date')\n",
        "test = test.sort_values('date')\n",
        "\n",
        "# Initialize PyCaret setup with corrected time series settings\n",
        "reg = setup(\n",
        "    data=train,\n",
        "    target='num_sold',\n",
        "    numeric_features=['year', 'month', 'day_of_week', 'day_of_month'],\n",
        "    categorical_features=['country', 'store', 'product', 'store_product'],\n",
        "    fold_strategy='timeseries',\n",
        "    fold=5,  # Number of folds for time series CV\n",
        "    fold_shuffle=False,  # Required for time series\n",
        "    data_split_shuffle=False,  # Required for time series\n",
        "    session_id=123,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Compare models suitable for time series\n",
        "models = compare_models(\n",
        "    n_select=5,  # Select top 5 models\n",
        "    sort='MAPE',\n",
        "    exclude=['dummy']  # Exclude baseline models\n",
        ")\n",
        "\n",
        "# Get the comparison results\n",
        "comparison_df = pull()\n",
        "\n",
        "# Create predictions for each model\n",
        "predictions = {}\n",
        "model_mape_scores = {}\n",
        "\n",
        "for i, model in enumerate(models, 1):\n",
        "    # Train the model\n",
        "    final_model = create_model(model)\n",
        "\n",
        "    # Make predictions on validation set\n",
        "    val_predictions = predict_model(final_model, data=get_config('X_train'))\n",
        "    mape = np.mean(np.abs((get_config('y_train') - val_predictions['prediction']) / get_config('y_train'))) * 100\n",
        "    model_name = f'Model_{i}_{type(model).__name__}'\n",
        "    model_mape_scores[model_name] = mape\n",
        "\n",
        "    # Make predictions on test set\n",
        "    test_pred = predict_model(final_model, data=test)\n",
        "    predictions[model_name] = test_pred['prediction'].values\n",
        "\n",
        "# Create submission DataFrame\n",
        "submissions = pd.DataFrame({'id': test['id']})\n",
        "for model_name, preds in predictions.items():\n",
        "    submissions[model_name] = preds\n",
        "\n",
        "# Print MAPE scores\n",
        "print(\"\\nModel MAPE Scores on Training Data:\")\n",
        "for model_name, mape in sorted(model_mape_scores.items(), key=lambda x: x[1]):\n",
        "    print(f\"{model_name}: {mape:.2f}%\")\n",
        "\n",
        "# Save predictions\n",
        "submissions.to_csv('sales_predictions.csv', index=False)\n",
        "\n",
        "# Get the best model\n",
        "best_model_name = min(model_mape_scores.items(), key=lambda x: x[1])[0]\n",
        "best_model_mape = model_mape_scores[best_model_name]\n",
        "print(f\"\\nBest Model: {best_model_name}\")\n",
        "print(f\"Best MAPE Score: {best_model_mape:.2f}%\")\n",
        "\n",
        "# Show sample predictions\n",
        "print(\"\\nSample Predictions:\")\n",
        "print(submissions.head())\n",
        "\n",
        "# Create ensemble prediction (average of all models)\n",
        "submissions['Ensemble_Prediction'] = submissions[[col for col in submissions.columns if col != 'id']].mean(axis=1)"
      ],
      "metadata": {
        "id": "ZXDNLCUUYy7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Best R2 finding based on 2 3 models with no preprossing"
      ],
      "metadata": {
        "id": "deaTfN3KY50s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression, ElasticNet\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load data\n",
        "train_df = pd.read_csv('/kaggle/input/new-new/train.csv')\n",
        "test_df = pd.read_csv('/kaggle/input/new-new/test.csv')\n",
        "\n",
        "# Prepare features and target\n",
        "X = train_df[['f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9']]\n",
        "y = train_df['target']\n",
        "X_test = test_df[['f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9']]\n",
        "\n",
        "# Split data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 1. Linear Regression\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "lr_pred = lr.predict(X_val)\n",
        "lr_score = r2_score(y_val, lr_pred)\n",
        "print(f\"Linear Regression R² Score: {lr_score:.4f}\")\n",
        "\n",
        "# 2. Elastic Net with Grid Search\n",
        "param_grid = {\n",
        "    'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
        "    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "}\n",
        "\n",
        "en = ElasticNet(max_iter=10000, random_state=42)\n",
        "grid_search = GridSearchCV(en, param_grid, scoring='r2', cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nElastic Net Best Parameters:\", grid_search.best_params_)\n",
        "print(f\"Elastic Net Best R² Score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Use the better model for final predictions\n",
        "if grid_search.best_score_ > lr_score:\n",
        "    print(\"\\nUsing Elastic Net for final predictions\")\n",
        "    best_model = grid_search.best_estimator_\n",
        "else:\n",
        "    print(\"\\nUsing Linear Regression for final predictions\")\n",
        "    best_model = lr\n",
        "\n",
        "# Generate predictions\n",
        "final_predictions = best_model.predict(X_test)\n",
        "\n",
        "# Create submission file\n",
        "submission = pd.DataFrame({\n",
        "    'id': test_df['id'],\n",
        "    'target': final_predictions\n",
        "})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"\\nSubmission file created: submission.csv\")\n",
        "\n",
        "# Display feature coefficients of the best model\n",
        "feature_names = ['f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9']\n",
        "coefficients = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Coefficient': best_model.coef_\n",
        "})\n",
        "print(\"\\nFeature Coefficients:\")\n",
        "print(coefficients.sort_values(by='Coefficient', key=abs, ascending=False))"
      ],
      "metadata": {
        "id": "Jt87AwzWZGfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A program to find result based on MAPE having stacking meta modal preprocessing hypertuning with optuna"
      ],
      "metadata": {
        "id": "q-qA10f6ZTSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "# Read the data\n",
        "train = pd.read_csv('/kaggle/input/new-new-new/train.csv')\n",
        "test = pd.read_csv('/kaggle/input/new-new-new/test.csv')\n",
        "\n",
        "# Remove rows with missing target values\n",
        "print(f\"Training rows before cleaning: {len(train)}\")\n",
        "train = train.dropna(subset=['num_sold'])\n",
        "print(f\"Training rows after cleaning: {len(train)}\")\n",
        "\n",
        "# Convert date to datetime and extract features\n",
        "train['date'] = pd.to_datetime(train['date'])\n",
        "test['date'] = pd.to_datetime(test['date'])\n",
        "\n",
        "# Create date features\n",
        "for df in [train, test]:\n",
        "    df['year'] = df['date'].dt.year\n",
        "    df['month'] = df['date'].dt.month\n",
        "    df['day_of_week'] = df['date'].dt.dayofweek\n",
        "    df['day_of_month'] = df['date'].dt.day\n",
        "    df['store_product'] = df['store'] + \"_\" + df['product']\n",
        "\n",
        "# Encode categorical variables\n",
        "categorical_features = ['country', 'store', 'product', 'store_product']\n",
        "encoders = {}\n",
        "\n",
        "for feature in categorical_features:\n",
        "    encoders[feature] = LabelEncoder()\n",
        "    train[feature] = encoders[feature].fit_transform(train[feature])\n",
        "    test[feature] = encoders[feature].transform(test[feature])\n",
        "\n",
        "# Prepare feature columns\n",
        "feature_columns = ['year', 'month', 'day_of_week', 'day_of_month'] + categorical_features\n",
        "\n",
        "# Prepare training data\n",
        "X = train[feature_columns]\n",
        "y = train['num_sold']\n",
        "\n",
        "# Initialize base models\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    max_features='sqrt',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "catboost = CatBoostRegressor(\n",
        "    iterations=1000,\n",
        "    learning_rate=0.03,\n",
        "    depth=6,\n",
        "    random_state=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Initialize meta-model\n",
        "meta_model = CatBoostRegressor(\n",
        "    iterations=500,\n",
        "    learning_rate=0.03,\n",
        "    depth=4,\n",
        "    random_state=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Create time series split for stacking\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "# Train base models and generate meta-features\n",
        "print(\"Training base models and generating meta-features...\")\n",
        "meta_features_train = np.zeros((len(X), 2))  # 2 base models\n",
        "meta_features_test = np.zeros((len(test), 2))\n",
        "\n",
        "# Train and predict with base models\n",
        "print(\"Training RF...\")\n",
        "rf.fit(X, y)\n",
        "meta_features_test[:, 0] = rf.predict(test[feature_columns])\n",
        "\n",
        "print(\"Training CatBoost...\")\n",
        "catboost.fit(X, y)\n",
        "meta_features_test[:, 1] = catboost.predict(test[feature_columns])\n",
        "\n",
        "# Generate meta-features for training\n",
        "for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):\n",
        "    print(f\"Processing fold {fold}/5...\")\n",
        "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_train_fold = y.iloc[train_idx]\n",
        "\n",
        "    # Train and predict with RF\n",
        "    rf_fold = RandomForestRegressor(\n",
        "        n_estimators=100,\n",
        "        max_features='sqrt',\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    rf_fold.fit(X_train_fold, y_train_fold)\n",
        "    meta_features_train[val_idx, 0] = rf_fold.predict(X_val_fold)\n",
        "\n",
        "    # Train and predict with CatBoost\n",
        "    catboost_fold = CatBoostRegressor(\n",
        "        iterations=1000,\n",
        "        learning_rate=0.03,\n",
        "        depth=6,\n",
        "        random_state=42,\n",
        "        verbose=False\n",
        "    )\n",
        "    catboost_fold.fit(X_train_fold, y_train_fold)\n",
        "    meta_features_train[val_idx, 1] = catboost_fold.predict(X_val_fold)\n",
        "\n",
        "# Train meta model\n",
        "print(\"Training meta model...\")\n",
        "meta_model.fit(meta_features_train, y)\n",
        "\n",
        "# Make final predictions\n",
        "print(\"Making final predictions...\")\n",
        "final_predictions = meta_model.predict(meta_features_test)\n",
        "\n",
        "# Create submission file\n",
        "submission = pd.DataFrame({\n",
        "    'id': test['id'],\n",
        "    'num_sold': final_predictions\n",
        "})\n",
        "\n",
        "# Ensure predictions are non-negative\n",
        "submission['num_sold'] = submission['num_sold'].clip(lower=0)\n",
        "\n",
        "# Save submission file\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "# Display sample of predictions\n",
        "print(\"\\nSample of submission file:\")\n",
        "print(submission.head())\n",
        "\n",
        "# Calculate and display validation scores\n",
        "print(\"\\nCalculating validation MAPE...\")\n",
        "meta_predictions = meta_model.predict(meta_features_train)\n",
        "mape = np.mean(np.abs((y - meta_predictions) / y)) * 100\n",
        "print(f\"Overall Validation MAPE: {mape:.2f}%\")\n",
        "\n",
        "# Analysis of model predictions\n",
        "print(\"\\nAnalyzing predictions...\")\n",
        "print(\"RF mean prediction:\", np.mean(meta_features_test[:, 0]))\n",
        "print(\"CatBoost mean prediction:\", np.mean(meta_features_test[:, 1]))\n",
        "print(\"Final stacking mean prediction:\", np.mean(final_predictions))"
      ],
      "metadata": {
        "id": "xI5AyV5NZgJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# code having feature engering and many more"
      ],
      "metadata": {
        "id": "LJsIWY-9aFlW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "# Read the data\n",
        "train = pd.read_csv('/kaggle/input/new-new-new/train.csv')\n",
        "test = pd.read_csv('/kaggle/input/new-new-new/test.csv')\n",
        "\n",
        "# Remove rows with missing target values\n",
        "print(f\"Training rows before cleaning: {len(train)}\")\n",
        "train = train.dropna(subset=['num_sold'])\n",
        "print(f\"Training rows after cleaning: {len(train)}\")\n",
        "\n",
        "# Convert date to datetime and extract features\n",
        "train['date'] = pd.to_datetime(train['date'])\n",
        "test['date'] = pd.to_datetime(test['date'])\n",
        "\n",
        "# Create date features\n",
        "for df in [train, test]:\n",
        "    df['year'] = df['date'].dt.year\n",
        "    df['month'] = df['date'].dt.month\n",
        "    df['day_of_week'] = df['date'].dt.dayofweek\n",
        "    df['day_of_month'] = df['date'].dt.day\n",
        "    df['store_product'] = df['store'] + \"_\" + df['product']\n",
        "\n",
        "# Encode categorical variables\n",
        "categorical_features = ['country', 'store', 'product', 'store_product']\n",
        "encoders = {}\n",
        "\n",
        "for feature in categorical_features:\n",
        "    encoders[feature] = LabelEncoder()\n",
        "    train[feature] = encoders[feature].fit_transform(train[feature])\n",
        "    test[feature] = encoders[feature].transform(test[feature])\n",
        "\n",
        "# Prepare feature columns\n",
        "feature_columns = ['year', 'month', 'day_of_week', 'day_of_month'] + categorical_features\n",
        "\n",
        "# Prepare training data\n",
        "X = train[feature_columns]\n",
        "y = train['num_sold']\n",
        "\n",
        "# Initialize base models\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    max_features='sqrt',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "catboost = CatBoostRegressor(\n",
        "    iterations=1000,\n",
        "    learning_rate=0.03,\n",
        "    depth=6,\n",
        "    random_state=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Initialize meta-model\n",
        "meta_model = CatBoostRegressor(\n",
        "    iterations=500,\n",
        "    learning_rate=0.03,\n",
        "    depth=4,\n",
        "    random_state=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Create time series split for stacking\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "# Train base models and generate meta-features\n",
        "print(\"Training base models and generating meta-features...\")\n",
        "meta_features_train = np.zeros((len(X), 2))  # 2 base models\n",
        "meta_features_test = np.zeros((len(test), 2))\n",
        "\n",
        "# Train and predict with base models\n",
        "print(\"Training RF...\")\n",
        "rf.fit(X, y)\n",
        "meta_features_test[:, 0] = rf.predict(test[feature_columns])\n",
        "\n",
        "print(\"Training CatBoost...\")\n",
        "catboost.fit(X, y)\n",
        "meta_features_test[:, 1] = catboost.predict(test[feature_columns])\n",
        "\n",
        "# Generate meta-features for training\n",
        "for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):\n",
        "    print(f\"Processing fold {fold}/5...\")\n",
        "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_train_fold = y.iloc[train_idx]\n",
        "\n",
        "    # Train and predict with RF\n",
        "    rf_fold = RandomForestRegressor(\n",
        "        n_estimators=100,\n",
        "        max_features='sqrt',\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    rf_fold.fit(X_train_fold, y_train_fold)\n",
        "    meta_features_train[val_idx, 0] = rf_fold.predict(X_val_fold)\n",
        "\n",
        "    # Train and predict with CatBoost\n",
        "    catboost_fold = CatBoostRegressor(\n",
        "        iterations=1000,\n",
        "        learning_rate=0.03,\n",
        "        depth=6,\n",
        "        random_state=42,\n",
        "        verbose=False\n",
        "    )\n",
        "    catboost_fold.fit(X_train_fold, y_train_fold)\n",
        "    meta_features_train[val_idx, 1] = catboost_fold.predict(X_val_fold)\n",
        "\n",
        "# Train meta model\n",
        "print(\"Training meta model...\")\n",
        "meta_model.fit(meta_features_train, y)\n",
        "\n",
        "# Make final predictions\n",
        "print(\"Making final predictions...\")\n",
        "final_predictions = meta_model.predict(meta_features_test)\n",
        "\n",
        "# Create submission file\n",
        "submission = pd.DataFrame({\n",
        "    'id': test['id'],\n",
        "    'num_sold': final_predictions\n",
        "})\n",
        "\n",
        "# Ensure predictions are non-negative\n",
        "submission['num_sold'] = submission['num_sold'].clip(lower=0)\n",
        "\n",
        "# Save submission file\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "# Display sample of predictions\n",
        "print(\"\\nSample of submission file:\")\n",
        "print(submission.head())\n",
        "\n",
        "# Calculate and display validation scores\n",
        "print(\"\\nCalculating validation MAPE...\")\n",
        "meta_predictions = meta_model.predict(meta_features_train)\n",
        "mape = np.mean(np.abs((y - meta_predictions) / y)) * 100\n",
        "print(f\"Overall Validation MAPE: {mape:.2f}%\")\n",
        "\n",
        "# Analysis of model predictions\n",
        "print(\"\\nAnalyzing predictions...\")\n",
        "print(\"RF mean prediction:\", np.mean(meta_features_test[:, 0]))\n",
        "print(\"CatBoost mean prediction:\", np.mean(meta_features_test[:, 1]))\n",
        "print(\"Final stacking mean prediction:\", np.mean(final_predictions))"
      ],
      "metadata": {
        "id": "JL4TN4G1aFMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Having hypertuning and ray tune"
      ],
      "metadata": {
        "id": "tReUTQBFaUyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "# Read the data\n",
        "train = pd.read_csv('/kaggle/input/new-new-new/train.csv')\n",
        "test = pd.read_csv('/kaggle/input/new-new-new/test.csv')\n",
        "\n",
        "# Remove rows with missing target values\n",
        "print(f\"Training rows before cleaning: {len(train)}\")\n",
        "train = train.dropna(subset=['num_sold'])\n",
        "print(f\"Training rows after cleaning: {len(train)}\")\n",
        "\n",
        "# Convert date to datetime and extract features\n",
        "train['date'] = pd.to_datetime(train['date'])\n",
        "test['date'] = pd.to_datetime(test['date'])\n",
        "\n",
        "# Create date features\n",
        "for df in [train, test]:\n",
        "    df['year'] = df['date'].dt.year\n",
        "    df['month'] = df['date'].dt.month\n",
        "    df['day_of_week'] = df['date'].dt.dayofweek\n",
        "    df['day_of_month'] = df['date'].dt.day\n",
        "    df['store_product'] = df['store'] + \"_\" + df['product']\n",
        "\n",
        "# Encode categorical variables\n",
        "categorical_features = ['country', 'store', 'product', 'store_product']\n",
        "encoders = {}\n",
        "\n",
        "for feature in categorical_features:\n",
        "    encoders[feature] = LabelEncoder()\n",
        "    train[feature] = encoders[feature].fit_transform(train[feature])\n",
        "    test[feature] = encoders[feature].transform(test[feature])\n",
        "\n",
        "# Prepare feature columns\n",
        "feature_columns = ['year', 'month', 'day_of_week', 'day_of_month'] + categorical_features\n",
        "\n",
        "# Prepare training data\n",
        "X = train[feature_columns]\n",
        "y = train['num_sold']\n",
        "\n",
        "# Initialize base models\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    max_features='sqrt',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "catboost = CatBoostRegressor(\n",
        "    iterations=1000,\n",
        "    learning_rate=0.03,\n",
        "    depth=6,\n",
        "    random_state=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Initialize meta-model\n",
        "meta_model = CatBoostRegressor(\n",
        "    iterations=500,\n",
        "    learning_rate=0.03,\n",
        "    depth=4,\n",
        "    random_state=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Create time series split for stacking\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "# Train base models and generate meta-features\n",
        "print(\"Training base models and generating meta-features...\")\n",
        "meta_features_train = np.zeros((len(X), 2))  # 2 base models\n",
        "meta_features_test = np.zeros((len(test), 2))\n",
        "\n",
        "# Train and predict with base models\n",
        "print(\"Training RF...\")\n",
        "rf.fit(X, y)\n",
        "meta_features_test[:, 0] = rf.predict(test[feature_columns])\n",
        "\n",
        "print(\"Training CatBoost...\")\n",
        "catboost.fit(X, y)\n",
        "meta_features_test[:, 1] = catboost.predict(test[feature_columns])\n",
        "\n",
        "# Generate meta-features for training\n",
        "for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):\n",
        "    print(f\"Processing fold {fold}/5...\")\n",
        "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_train_fold = y.iloc[train_idx]\n",
        "\n",
        "    # Train and predict with RF\n",
        "    rf_fold = RandomForestRegressor(\n",
        "        n_estimators=100,\n",
        "        max_features='sqrt',\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    rf_fold.fit(X_train_fold, y_train_fold)\n",
        "    meta_features_train[val_idx, 0] = rf_fold.predict(X_val_fold)\n",
        "\n",
        "    # Train and predict with CatBoost\n",
        "    catboost_fold = CatBoostRegressor(\n",
        "        iterations=1000,\n",
        "        learning_rate=0.03,\n",
        "        depth=6,\n",
        "        random_state=42,\n",
        "        verbose=False\n",
        "    )\n",
        "    catboost_fold.fit(X_train_fold, y_train_fold)\n",
        "    meta_features_train[val_idx, 1] = catboost_fold.predict(X_val_fold)\n",
        "\n",
        "# Train meta model\n",
        "print(\"Training meta model...\")\n",
        "meta_model.fit(meta_features_train, y)\n",
        "\n",
        "# Make final predictions\n",
        "print(\"Making final predictions...\")\n",
        "final_predictions = meta_model.predict(meta_features_test)\n",
        "\n",
        "# Create submission file\n",
        "submission = pd.DataFrame({\n",
        "    'id': test['id'],\n",
        "    'num_sold': final_predictions\n",
        "})\n",
        "\n",
        "# Ensure predictions are non-negative\n",
        "submission['num_sold'] = submission['num_sold'].clip(lower=0)\n",
        "\n",
        "# Save submission file\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "# Display sample of predictions\n",
        "print(\"\\nSample of submission file:\")\n",
        "print(submission.head())\n",
        "\n",
        "# Calculate and display validation scores\n",
        "print(\"\\nCalculating validation MAPE...\")\n",
        "meta_predictions = meta_model.predict(meta_features_train)\n",
        "mape = np.mean(np.abs((y - meta_predictions) / y)) * 100\n",
        "print(f\"Overall Validation MAPE: {mape:.2f}%\")\n",
        "\n",
        "# Analysis of model predictions\n",
        "print(\"\\nAnalyzing predictions...\")\n",
        "print(\"RF mean prediction:\", np.mean(meta_features_test[:, 0]))\n",
        "print(\"CatBoost mean prediction:\", np.mean(meta_features_test[:, 1]))\n",
        "print(\"Final stacking mean prediction:\", np.mean(final_predictions))"
      ],
      "metadata": {
        "id": "xQpj_tVfaZO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from typing import Tuple, Dict, Any\n",
        "import warnings\n",
        "import joblib\n",
        "from pathlib import Path\n",
        "\n",
        "# Ray Tune imports\n",
        "from ray import tune\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "from ray.tune.search.hyperopt import HyperOptSearch\n",
        "\n",
        "# Hyperopt imports\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
        "from hyperopt.pyll import scope\n",
        "\n",
        "class MLPipelineRayTune:\n",
        "    def __init__(self, random_state: int = 42):\n",
        "        self.random_state = random_state\n",
        "        self.scaler = StandardScaler()\n",
        "        self.model = None\n",
        "        self.features = None\n",
        "\n",
        "    def load_and_preprocess_data(self, train_path: str, test_path: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "        \"\"\"Load and preprocess data in one step\"\"\"\n",
        "        dtype_dict = {f'f{i}': np.float32 for i in range(300)}\n",
        "\n",
        "        train_df = pd.read_csv(train_path, dtype=dtype_dict)\n",
        "        test_df = pd.read_csv(test_path, dtype=dtype_dict)\n",
        "\n",
        "        self.features = [col for col in train_df.columns if col.startswith('f')]\n",
        "        if not self.features:\n",
        "            raise ValueError(\"No feature columns found\")\n",
        "\n",
        "        X_train = train_df[self.features].values\n",
        "        y_train = train_df['target'].values\n",
        "        X_test = test_df[self.features].values\n",
        "\n",
        "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
        "        X_test_scaled = self.scaler.transform(X_test)\n",
        "\n",
        "        return X_train_scaled, y_train, X_test_scaled\n",
        "\n",
        "    def train_ray_tune(self, X_train: np.ndarray, y_train: np.ndarray, num_samples: int = 50, max_epochs: int = 100) -> Dict:\n",
        "        \"\"\"Train model using Ray Tune with ASHA scheduler and HyperOpt algorithm\"\"\"\n",
        "\n",
        "        def objective(config):\n",
        "            from ray import train\n",
        "\n",
        "            et = ExtraTreesRegressor(\n",
        "                n_estimators=int(config[\"n_estimators\"]),\n",
        "                max_depth=int(config[\"max_depth\"]),\n",
        "                min_samples_split=int(config[\"min_samples_split\"]),\n",
        "                min_samples_leaf=int(config[\"min_samples_leaf\"]),\n",
        "                max_features=config[\"max_features\"],\n",
        "                random_state=self.random_state,\n",
        "                n_jobs=1\n",
        "            )\n",
        "\n",
        "            scores = cross_val_score(\n",
        "                et, X_train, y_train,\n",
        "                cv=KFold(n_splits=5, shuffle=True, random_state=self.random_state),\n",
        "                scoring='r2',\n",
        "                n_jobs=1\n",
        "            )\n",
        "\n",
        "            train.report({\"mean_r2\": scores.mean()})\n",
        "\n",
        "        # Define search space\n",
        "        search_space = {\n",
        "            \"n_estimators\": tune.randint(50, 500),\n",
        "            \"max_depth\": tune.randint(5, 50),\n",
        "            \"min_samples_split\": tune.randint(2, 10),\n",
        "            \"min_samples_leaf\": tune.randint(1, 5),\n",
        "            \"max_features\": tune.choice(['sqrt', 'log2', None]),\n",
        "        }\n",
        "\n",
        "        # Initialize ASHA scheduler\n",
        "        asha_scheduler = ASHAScheduler(\n",
        "            time_attr='training_iteration',\n",
        "            metric=\"mean_r2\",\n",
        "            mode=\"max\",\n",
        "            max_t=max_epochs,\n",
        "            grace_period=5,\n",
        "            reduction_factor=3\n",
        "        )\n",
        "\n",
        "        # Initialize HyperOpt search algorithm\n",
        "        hyperopt_search = HyperOptSearch(\n",
        "            metric=\"mean_r2\",\n",
        "            mode=\"max\"\n",
        "        )\n",
        "\n",
        "        # Run optimization\n",
        "        analysis = tune.run(\n",
        "            objective,\n",
        "            config=search_space,\n",
        "            search_alg=hyperopt_search,\n",
        "            scheduler=asha_scheduler,\n",
        "            num_samples=num_samples,\n",
        "            resources_per_trial={\"cpu\": 4}\n",
        "        )\n",
        "\n",
        "        best_config = analysis.get_best_config(metric=\"mean_r2\", mode=\"max\")\n",
        "\n",
        "        # Train final model with best parameters\n",
        "        self.model = ExtraTreesRegressor(\n",
        "            **{k: int(v) if isinstance(v, float) and k != 'max_features' else v\n",
        "               for k, v in best_config.items()},\n",
        "            random_state=self.random_state,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        self.model.fit(X_train, y_train)\n",
        "\n",
        "        return best_config\n",
        "\n",
        "class MLPipelineHyperopt:\n",
        "    def __init__(self, random_state: int = 42):\n",
        "        self.random_state = random_state\n",
        "        self.scaler = StandardScaler()\n",
        "        self.model = None\n",
        "        self.features = None\n",
        "\n",
        "    def load_and_preprocess_data(self, train_path: str, test_path: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "        \"\"\"Load and preprocess data\"\"\"\n",
        "        dtype_dict = {f'f{i}': np.float32 for i in range(300)}\n",
        "\n",
        "        train_df = pd.read_csv(train_path, dtype=dtype_dict)\n",
        "        test_df = pd.read_csv(test_path, dtype=dtype_dict)\n",
        "\n",
        "        self.features = [col for col in train_df.columns if col.startswith('f')]\n",
        "        if not self.features:\n",
        "            raise ValueError(\"No feature columns found\")\n",
        "\n",
        "        X_train = train_df[self.features].values\n",
        "        y_train = train_df['target'].values\n",
        "        X_test = test_df[self.features].values\n",
        "\n",
        "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
        "        X_test_scaled = self.scaler.transform(X_test)\n",
        "\n",
        "        return X_train_scaled, y_train, X_test_scaled\n",
        "\n",
        "    def train_hyperopt(self, X_train: np.ndarray, y_train: np.ndarray, max_evals: int = 100) -> Dict:\n",
        "        \"\"\"Train model using Hyperopt with TPE algorithm\"\"\"\n",
        "\n",
        "        def objective(params):\n",
        "            et = ExtraTreesRegressor(\n",
        "                n_estimators=int(params[\"n_estimators\"]),\n",
        "                max_depth=int(params[\"max_depth\"]),\n",
        "                min_samples_split=int(params[\"min_samples_split\"]),\n",
        "                min_samples_leaf=int(params[\"min_samples_leaf\"]),\n",
        "                max_features=params[\"max_features\"],\n",
        "                random_state=self.random_state,\n",
        "                n_jobs=-1\n",
        "            )\n",
        "\n",
        "            scores = cross_val_score(\n",
        "                et, X_train, y_train,\n",
        "                cv=KFold(n_splits=5, shuffle=True, random_state=self.random_state),\n",
        "                scoring='r2'\n",
        "            )\n",
        "\n",
        "            return {'loss': -scores.mean(), 'status': STATUS_OK}\n",
        "\n",
        "        space = {\n",
        "            'n_estimators': scope.int(hp.quniform('n_estimators', 100, 1000, 10)),\n",
        "            'max_depth': scope.int(hp.quniform('max_depth', 10, 100, 1)),\n",
        "            'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 20, 1)),\n",
        "            'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 10, 1)),\n",
        "            'max_features': hp.choice('max_features', ['sqrt', 'log2', None])\n",
        "        }\n",
        "\n",
        "        trials = Trials()\n",
        "        best = fmin(\n",
        "            fn=objective,\n",
        "            space=space,\n",
        "            algo=tpe.suggest,\n",
        "            max_evals=max_evals,\n",
        "            trials=trials,\n",
        "            show_progressbar=True\n",
        "        )\n",
        "\n",
        "        best_params = {\n",
        "            'n_estimators': int(best['n_estimators']),\n",
        "            'max_depth': int(best['max_depth']),\n",
        "            'min_samples_split': int(best['min_samples_split']),\n",
        "            'min_samples_leaf': int(best['min_samples_leaf']),\n",
        "            'max_features': ['sqrt', 'log2', None][best['max_features']]\n",
        "        }\n",
        "\n",
        "        self.model = ExtraTreesRegressor(\n",
        "            **best_params,\n",
        "            random_state=self.random_state,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        self.model.fit(X_train, y_train)\n",
        "\n",
        "        return best_params\n",
        "\n",
        "def main():\n",
        "    warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "    try:\n",
        "        # Example usage of Ray Tune version\n",
        "        ray_pipeline = MLPipelineRayTune()\n",
        "        X_train, y_train, X_test = ray_pipeline.load_and_preprocess_data('/kaggle/input/kaggle/train.csv', '/kaggle/input/kaggle/test.csv')\n",
        "        best_params_ray = ray_pipeline.train_ray_tune(X_train, y_train)\n",
        "        print(\"Ray Tune best parameters:\", best_params_ray)\n",
        "\n",
        "        # Example usage of Hyperopt version\n",
        "        hyperopt_pipeline = MLPipelineHyperopt()\n",
        "        X_train, y_train, X_test = hyperopt_pipeline.load_and_preprocess_data('/kaggle/input/kaggle/train.csv', '/kaggle/input/kaggle/test.csv')\n",
        "        best_params_hyperopt = hyperopt_pipeline.train_hyperopt(X_train, y_train)\n",
        "        print(\"Hyperopt best parameters:\", best_params_hyperopt)\n",
        "\n",
        "        return ray_pipeline, hyperopt_pipeline\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ray_pipeline, hyperopt_pipeline = main()"
      ],
      "metadata": {
        "id": "Rtb7jcZEdnA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysing the coding"
      ],
      "metadata": {
        "id": "v4tDp2VrafKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler ,RobustScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "print(\"Packages Imported \")"
      ],
      "metadata": {
        "id": "kmkmlXoXae8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data= pd.read_csv('/kaggle/input/noicef/train.csv')\n",
        "test_data= pd.read_csv('/kaggle/input/noicef/test.csv')\n",
        "sample_sub = pd.read_csv('/kaggle/input/noicef/sample_submission.csv')\n",
        "print(\"Data imported\")"
      ],
      "metadata": {
        "id": "MIaHbc09arSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "id": "aGcoeXyIatNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of rows: {train_data.shape[0]};  Number of columns: {train_data.shape[1]}; No of missing values: {sum(train_data.isna().sum())}')"
      ],
      "metadata": {
        "id": "mmWx3-SPavPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.describe().style.background_gradient(cmap='coolwarm')"
      ],
      "metadata": {
        "id": "ua_5_GbSaxnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# variables variaition\n",
        "df_var=train_data.var().reset_index()\n",
        "df_var.columns =['feature', 'variation']\n",
        "df_var.sort_values(\"variation\",ascending = True)"
      ],
      "metadata": {
        "id": "VAPOGUBxa13e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlationmatrix\n",
        "corrMatrix =train_data.corr(method='pearson', min_periods=1)\n",
        "corrMatrix"
      ],
      "metadata": {
        "id": "UkW3JLs6a448"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cor_targ = train_data.corrwith(train_data[\"target\"]).reset_index()\n",
        "cor_targ.columns =['feature', 'CorrelatioWithTarget']\n",
        "cor_targ.sort_values('CorrelatioWithTarget',ascending = False)"
      ],
      "metadata": {
        "id": "mE9EGMz8a6_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using simple stacking of models"
      ],
      "metadata": {
        "id": "cwYCY32HbR7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import StackingRegressor, ExtraTreesRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv('/kaggle/input/noicef/train.csv')\n",
        "test = pd.read_csv('/kaggle/input/noicef/test.csv')\n",
        "\n",
        "# Separate features and target\n",
        "X = train.drop(['target'], axis=1)\n",
        "y = train['target']\n",
        "test_features = test.drop('id', axis=1)\n",
        "\n",
        "# Define base models\n",
        "extra_trees = ExtraTreesRegressor(n_estimators=300, random_state=42)\n",
        "catboost = CatBoostRegressor(iterations=500, learning_rate=0.1, depth=8, verbose=0, random_state=42)\n",
        "\n",
        "# Define meta-model\n",
        "meta_model = Ridge(alpha=1.0)\n",
        "\n",
        "# Create a stacking regressor\n",
        "stacking_regressor = StackingRegressor(\n",
        "    estimators=[('extra_trees', extra_trees), ('catboost', catboost)],\n",
        "    final_estimator=meta_model,\n",
        "    cv=20\n",
        ")\n",
        "\n",
        "# Optional: Create a validation set to check performance\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the stacking regressor\n",
        "stacking_regressor.fit(X, y)  # Use full dataset for final training\n",
        "\n",
        "# Make predictions on test set\n",
        "test_predictions = stacking_regressor.predict(test_features)\n",
        "\n",
        "# Create submission DataFrame\n",
        "submission = pd.DataFrame({\n",
        "    'id': test['id'],\n",
        "    'target': test_predictions\n",
        "})\n",
        "\n",
        "# Save submission file\n",
        "submission.to_csv('subbbbbmissssion1.csv', index=False)\n",
        "\n",
        "# Optional: Print validation score if using train-validation split\n",
        "val_predictions = stacking_regressor.predict(X_val)\n",
        "print(f\"Validation R2 Score: {r2_score(y_val, val_predictions)}\")"
      ],
      "metadata": {
        "id": "-WR5tCtjbWWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import StackingRegressor, ExtraTreesRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv('/kaggle/input/noicef/train.csv')\n",
        "test = pd.read_csv('/kaggle/input/noicef/test.csv')\n",
        "\n",
        "# Separate features and target\n",
        "X = train.drop(['target'], axis=1)\n",
        "y = train['target']\n",
        "test_features = test.drop('id', axis=1)\n",
        "\n",
        "# Define base models\n",
        "extra_trees = ExtraTreesRegressor(n_estimators=10000, random_state=42)\n",
        "catboost = CatBoostRegressor(iterations=20000, learning_rate=0.1, depth=8, verbose=0, random_state=42)\n",
        "\n",
        "# Define SVR meta-model\n",
        "svr_meta = SVR(kernel='rbf', C=10.0, epsilon=0.1)\n",
        "\n",
        "try:\n",
        "    # Create and train stacking regressor with SVR meta-model\n",
        "    stacking_regressor = StackingRegressor(\n",
        "        estimators=[('extra_trees', extra_trees), ('catboost', catboost)],\n",
        "        final_estimator=svr_meta,\n",
        "        cv=10\n",
        "    )\n",
        "\n",
        "    # Fit the model on the entire dataset\n",
        "    stacking_regressor.fit(X, y)\n",
        "\n",
        "    # Make predictions on the entire dataset\n",
        "    predictions = stacking_regressor.predict(X)\n",
        "\n",
        "    # Calculate R2 score on the entire dataset\n",
        "    full_score = r2_score(y, predictions)\n",
        "    print(f\"SVR Meta-model - Full Dataset R2 Score: {full_score:.4f}\")\n",
        "\n",
        "    # Generate predictions for test set\n",
        "    test_predictions = stacking_regressor.predict(test_features)\n",
        "\n",
        "    # Create submission DataFrame\n",
        "    submission = pd.DataFrame({\n",
        "        'id': test['id'],\n",
        "        'target': test_predictions\n",
        "    })\n",
        "\n",
        "    # Save submission file\n",
        "    submission.to_csv('submission_svr_meta7.csv', index=False)\n",
        "    print(\"\\nSubmission file created successfully\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {str(e)}\")"
      ],
      "metadata": {
        "id": "R_m4glF7bv37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import StackingRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv('/kaggle/input/noicef/train.csv')\n",
        "test = pd.read_csv('/kaggle/input/noicef/test.csv')\n",
        "\n",
        "# Separate features and target\n",
        "X = train.drop(['target'], axis=1)\n",
        "y = train['target']\n",
        "test_features = test.drop('id', axis=1)\n",
        "\n",
        "# Scale the features for KNN\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "test_features_scaled = scaler.transform(test_features)\n",
        "\n",
        "# Define base models\n",
        "et_model = ExtraTreesRegressor(n_estimators=3000, random_state=42)\n",
        "xgb_model = XGBRegressor(\n",
        "    n_estimators=3000,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=8,\n",
        "    random_state=42\n",
        ")\n",
        "catboost_model = CatBoostRegressor(\n",
        "    iterations=5000,\n",
        "    learning_rate=0.1,\n",
        "    depth=8,\n",
        "    verbose=0,\n",
        "    random_state=42\n",
        ")\n",
        "gbr_model = GradientBoostingRegressor(\n",
        "    n_estimators=3000,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=8,\n",
        "    random_state=42\n",
        ")\n",
        "knn_model = KNeighborsRegressor(\n",
        "    n_neighbors=5,\n",
        "    weights='distance',\n",
        "    metric='minkowski',\n",
        "    p=2  # Euclidean distance\n",
        ")\n",
        "\n",
        "# Define meta-model (final estimator)\n",
        "meta_model = SVR(kernel='rbf', C=10.0, epsilon=0.1)\n",
        "\n",
        "try:\n",
        "    # Create stacking regressor\n",
        "    stacking_model = StackingRegressor(\n",
        "        estimators=[\n",
        "            ('et', et_model),\n",
        "            ('xgb', xgb_model),\n",
        "            ('catboost', catboost_model),\n",
        "            ('gbr', gbr_model),\n",
        "            ('knn', knn_model)\n",
        "        ],\n",
        "        final_estimator=meta_model,\n",
        "        cv=6\n",
        "    )\n",
        "\n",
        "    # Train the stacking model on the training data\n",
        "    # Use scaled data for KNN but original data for tree-based models\n",
        "    stacking_model.fit(X_scaled, y)\n",
        "\n",
        "    # Make predictions on the entire dataset\n",
        "    predictions = stacking_model.predict(X_scaled)\n",
        "\n",
        "    # Calculate R2 score on the entire dataset\n",
        "    full_score = r2_score(y, predictions)\n",
        "    print(f\"Stacking Model - Full Dataset R2 Score: {full_score:.4f}\")\n",
        "\n",
        "    # Generate predictions for test set\n",
        "    test_predictions = stacking_model.predict(test_features_scaled)\n",
        "\n",
        "    # Create submission DataFrame\n",
        "    submission = pd.DataFrame({\n",
        "        'id': test['id'],\n",
        "        'target': test_predictions\n",
        "    })\n",
        "\n",
        "    # Save submission file\n",
        "    submission.to_csv('submission_stacking_with_knn.csv', index=False)\n",
        "    print(\"\\nSubmission file created successfully\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {str(e)}\")"
      ],
      "metadata": {
        "id": "wCfB0nlPb0bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import StackingRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv('/kaggle/input/new-data/ucs-654-kaggle-hack-lab-exam-1/train.csv')\n",
        "test = pd.read_csv('/kaggle/input/new-data/ucs-654-kaggle-hack-lab-exam-1/test.csv')\n",
        "\n",
        "# Separate features and target\n",
        "X = train.drop(['target'], axis=1)\n",
        "y = train['target']\n",
        "test_features = test.drop('id', axis=1)\n",
        "\n",
        "# Define base models\n",
        "et_model = ExtraTreesRegressor(n_estimators=2000, random_state=42)\n",
        "xgb_model = XGBRegressor(\n",
        "    n_estimators=2000,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=7,\n",
        "    random_state=42\n",
        ")\n",
        "catboost_model = CatBoostRegressor(\n",
        "    iterations=5000,\n",
        "    learning_rate=0.1,\n",
        "    depth=9,\n",
        "    verbose=0,\n",
        "    random_state=42\n",
        ")\n",
        "gbr_model = GradientBoostingRegressor(\n",
        "    n_estimators=2000,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=7,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define meta-model (final estimator)\n",
        "meta_model = SVR(kernel='rbf', C=10.0, epsilon=0.1)\n",
        "\n",
        "try:\n",
        "    # Create stacking regressor\n",
        "    stacking_model = StackingRegressor(\n",
        "        estimators=[\n",
        "            ('et', et_model),\n",
        "            ('xgb', xgb_model),\n",
        "            ('catboost', catboost_model),\n",
        "            ('gbr', gbr_model)\n",
        "        ],\n",
        "        final_estimator=meta_model,\n",
        "        cv=7  # As shown in the image\n",
        "    )\n",
        "\n",
        "    # Train the stacking model on the training data\n",
        "    stacking_model.fit(X, y)\n",
        "\n",
        "    # Make predictions on the entire dataset\n",
        "    predictions = stacking_model.predict(X)\n",
        "\n",
        "    # Calculate R2 score on the entire dataset\n",
        "    full_score = r2_score(y, predictions)\n",
        "    print(f\"Stacking Model - Full Dataset R2 Score: {full_score:.4f}\")\n",
        "\n",
        "    # Generate predictions for test set\n",
        "    test_predictions = stacking_model.predict(test_features)\n",
        "\n",
        "    # Create submission DataFrame\n",
        "    submission = pd.DataFrame({\n",
        "        'id': test['id'],\n",
        "        'target': test_predictions\n",
        "    })\n",
        "\n",
        "    # Save submission file\n",
        "    submission.to_csv('submission_stacking1.csv', index=False)\n",
        "    print(\"\\nSubmission file created successfully\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {str(e)}\")"
      ],
      "metadata": {
        "id": "845nSsd4b28P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import StackingRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv('/kaggle/input/new-data/ucs-654-kaggle-hack-lab-exam-1/train.csv')\n",
        "test = pd.read_csv('/kaggle/input/new-data/ucs-654-kaggle-hack-lab-exam-1/test.csv')\n",
        "\n",
        "# Separate features and target\n",
        "X = train.drop(['target'], axis=1)\n",
        "y = train['target']\n",
        "test_features = test.drop('id', axis=1)\n",
        "\n",
        "# Scale the features for KNN\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "test_features_scaled = scaler.transform(test_features)\n",
        "\n",
        "# Define base models\n",
        "et_model = ExtraTreesRegressor(n_estimators=3000, random_state=42)\n",
        "xgb_model = XGBRegressor(\n",
        "    n_estimators=3000,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=8,\n",
        "    random_state=42\n",
        ")\n",
        "catboost_model = CatBoostRegressor(\n",
        "    iterations=5000,\n",
        "    learning_rate=0.1,\n",
        "    depth=8,\n",
        "    verbose=0,\n",
        "    random_state=42\n",
        ")\n",
        "gbr_model = GradientBoostingRegressor(\n",
        "    n_estimators=3000,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=8,\n",
        "    random_state=42\n",
        ")\n",
        "knn_model = KNeighborsRegressor(\n",
        "    n_neighbors=5,\n",
        "    weights='distance',\n",
        "    metric='minkowski',\n",
        "    p=2\n",
        ")\n",
        "lgbm_model = LGBMRegressor(\n",
        "    n_estimators=3000,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=8,\n",
        "    num_leaves=31,\n",
        "    boosting_type='gbdt',\n",
        "    random_state=42,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "# Define meta-model (final estimator)\n",
        "meta_model = SVR(kernel='rbf', C=10.0, epsilon=0.1)\n",
        "\n",
        "try:\n",
        "    # Create stacking regressor\n",
        "    stacking_model = StackingRegressor(\n",
        "        estimators=[\n",
        "            ('et', et_model),\n",
        "            ('xgb', xgb_model),\n",
        "            ('catboost', catboost_model),\n",
        "            ('gbr', gbr_model),\n",
        "            ('knn', knn_model),\n",
        "            ('lgbm', lgbm_model)\n",
        "        ],\n",
        "        final_estimator=meta_model,\n",
        "        cv=6\n",
        "    )\n",
        "\n",
        "    # Train the stacking model on the training data\n",
        "    stacking_model.fit(X_scaled, y)\n",
        "\n",
        "    # Make predictions on the entire dataset\n",
        "    predictions = stacking_model.predict(X_scaled)\n",
        "\n",
        "    # Calculate R2 score on the entire dataset\n",
        "    full_score = r2_score(y, predictions)\n",
        "    print(f\"Stacking Model - Full Dataset R2 Score: {full_score:.4f}\")\n",
        "\n",
        "    # Generate predictions for test set\n",
        "    test_predictions = stacking_model.predict(test_features_scaled)\n",
        "\n",
        "    # Create submission DataFrame\n",
        "    submission = pd.DataFrame({\n",
        "        'id': test['id'],\n",
        "        'target': test_predictions\n",
        "    })\n",
        "\n",
        "    # Save submission file\n",
        "    submission.to_csv('submission_stacking_with_knn_lgbm.csv', index=False)\n",
        "    print(\"\\nSubmission file created successfully\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {str(e)}\")"
      ],
      "metadata": {
        "id": "VdvWMnFacUL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using simple stacking of models and also checking best meta model"
      ],
      "metadata": {
        "id": "hUCHCEqqbdxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import StackingRegressor, ExtraTreesRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import r2_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv('/kaggle/input/noicef/train.csv')\n",
        "test = pd.read_csv('/kaggle/input/noicef/test.csv')\n",
        "\n",
        "# Separate features and target\n",
        "X = train.drop(['target'], axis=1)\n",
        "y = train['target']\n",
        "test_features = test.drop('id', axis=1)\n",
        "\n",
        "# Define base models\n",
        "extra_trees = ExtraTreesRegressor(n_estimators=300, random_state=42)\n",
        "catboost = CatBoostRegressor(iterations=500, learning_rate=0.1, depth=8, verbose=0, random_state=42)\n",
        "\n",
        "# Define different meta-models to try\n",
        "meta_models = {\n",
        "    'ridge': Ridge(alpha=1.0),\n",
        "    'lasso': Lasso(alpha=0.01),\n",
        "    'elastic_net': ElasticNet(alpha=0.01, l1_ratio=0.5),\n",
        "    'svr': SVR(kernel='rbf', C=1.0, epsilon=0.1),\n",
        "    'xgboost': xgb.XGBRegressor(\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=4,\n",
        "        random_state=42\n",
        "    )\n",
        "}\n",
        "\n",
        "# Create train-validation split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Dictionary to store results\n",
        "results = {}\n",
        "\n",
        "# Test each meta-model\n",
        "for name, meta_model in meta_models.items():\n",
        "    # Create and train stacking regressor\n",
        "    stacking_regressor = StackingRegressor(\n",
        "        estimators=[('extra_trees', extra_trees), ('catboost', catboost)],\n",
        "        final_estimator=meta_model,\n",
        "        cv=10\n",
        "    )\n",
        "\n",
        "    # Fit the model\n",
        "    stacking_regressor.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on validation set\n",
        "    val_predictions = stacking_regressor.predict(X_val)\n",
        "    val_score = r2_score(y_val, val_predictions)\n",
        "\n",
        "    # Store results\n",
        "    results[name] = {\n",
        "        'r2_score': val_score,\n",
        "        'model': stacking_regressor\n",
        "    }\n",
        "\n",
        "    print(f\"{name.upper()} - Validation R2 Score: {val_score:.4f}\")\n",
        "\n",
        "# Find best performing model\n",
        "best_model_name = max(results.items(), key=lambda x: x[1]['r2_score'])[0]\n",
        "best_model = results[best_model_name]['model']\n",
        "print(f\"\\nBest performing meta-model: {best_model_name}\")\n",
        "\n",
        "# Retrain best model on full dataset\n",
        "best_stacking_regressor = StackingRegressor(\n",
        "    estimators=[('extra_trees', extra_trees), ('catboost', catboost)],\n",
        "    final_estimator=meta_models[best_model_name],\n",
        "    cv=10\n",
        ")\n",
        "best_stacking_regressor.fit(X, y)\n",
        "\n",
        "# Make predictions on test set\n",
        "test_predictions = best_stacking_regressor.predict(test_features)\n",
        "\n",
        "# Create submission DataFrame\n",
        "submission = pd.DataFrame({\n",
        "    'id': test['id'],\n",
        "    'target': test_predictions\n",
        "})\n",
        "\n",
        "# Save submission file\n",
        "submission.to_csv('submission_best_meta.csv', index=False)"
      ],
      "metadata": {
        "id": "eXJjKeVEbYKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using simple stacking of models and with multiple meta models"
      ],
      "metadata": {
        "id": "-bYSWohzcfta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import (\n",
        "    StackingRegressor,\n",
        "    ExtraTreesRegressor,\n",
        "    GradientBoostingRegressor,\n",
        "    RandomForestRegressor\n",
        ")\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv('/kaggle/input/new-data/ucs-654-kaggle-hack-lab-exam-1/train.csv')\n",
        "test = pd.read_csv('/kaggle/input/new-data/ucs-654-kaggle-hack-lab-exam-1/test.csv')\n",
        "\n",
        "# Separate features and target\n",
        "X = train.drop(['target'], axis=1)\n",
        "y = train['target']\n",
        "test_features = test.drop('id', axis=1)\n",
        "\n",
        "# Scale the features for KNN\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "test_features_scaled = scaler.transform(test_features)\n",
        "\n",
        "# Define base models\n",
        "et_model = ExtraTreesRegressor(n_estimators=3000, random_state=42)\n",
        "xgb_model = XGBRegressor(\n",
        "    n_estimators=3000,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=8,\n",
        "    random_state=42\n",
        ")\n",
        "catboost_model = CatBoostRegressor(\n",
        "    iterations=5000,\n",
        "    learning_rate=0.1,\n",
        "    depth=8,\n",
        "    verbose=0,\n",
        "    random_state=42\n",
        ")\n",
        "gbr_model = GradientBoostingRegressor(\n",
        "    n_estimators=3000,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=8,\n",
        "    random_state=42\n",
        ")\n",
        "knn_model = KNeighborsRegressor(\n",
        "    n_neighbors=5,\n",
        "    weights='distance',\n",
        "    metric='minkowski',\n",
        "    p=2\n",
        ")\n",
        "lgbm_model = LGBMRegressor(\n",
        "    n_estimators=3000,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=8,\n",
        "    num_leaves=31,\n",
        "    boosting_type='gbdt',\n",
        "    random_state=42,\n",
        "    verbose=-1\n",
        ")\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=3000,\n",
        "    max_depth=8,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Define meta-models (trying both XGBoost and LightGBM as meta-learners)\n",
        "meta_model_xgb = XGBRegressor(\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.01,\n",
        "    max_depth=4,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "meta_model_lgbm = LGBMRegressor(\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.01,\n",
        "    max_depth=4,\n",
        "    num_leaves=15,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "# Try both meta-models\n",
        "for meta_model, model_name in [(meta_model_xgb, 'xgb'), (meta_model_lgbm, 'lgbm')]:\n",
        "    try:\n",
        "        # Create stacking regressor\n",
        "        stacking_model = StackingRegressor(\n",
        "            estimators=[\n",
        "                ('et', et_model),\n",
        "                ('xgb', xgb_model),\n",
        "                ('catboost', catboost_model),\n",
        "                ('gbr', gbr_model),\n",
        "                ('knn', knn_model),\n",
        "                ('lgbm', lgbm_model),\n",
        "                ('rf', rf_model)\n",
        "            ],\n",
        "            final_estimator=meta_model,\n",
        "            cv=6\n",
        "        )\n",
        "\n",
        "        # Train the stacking model on the training data\n",
        "        stacking_model.fit(X_scaled, y)\n",
        "\n",
        "        # Make predictions on the entire dataset\n",
        "        predictions = stacking_model.predict(X_scaled)\n",
        "\n",
        "        # Calculate R2 score on the entire dataset\n",
        "        full_score = r2_score(y, predictions)\n",
        "        print(f\"Stacking Model with {model_name.upper()} meta-learner - Full Dataset R2 Score: {full_score:.4f}\")\n",
        "\n",
        "        # Generate predictions for test set\n",
        "        test_predictions = stacking_model.predict(test_features_scaled)\n",
        "\n",
        "        # Create submission DataFrame\n",
        "        submission = pd.DataFrame({\n",
        "            'id': test['id'],\n",
        "            'target': test_predictions\n",
        "        })\n",
        "\n",
        "        # Save submission file\n",
        "        submission.to_csv(f'submission_stacking_meta_{model_name}.csv', index=False)\n",
        "        print(f\"\\nSubmission file for {model_name.upper()} meta-learner created successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error with {model_name} meta-learner: {str(e)}\")"
      ],
      "metadata": {
        "id": "MtlmuCtJcfUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using h2o for finding models instead of pycaret"
      ],
      "metadata": {
        "id": "Vtr-7bqwdSHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h2o\n",
        "from h2o.automl import H2OAutoML\n",
        "\n",
        "# Initialize H2O\n",
        "h2o.init()\n",
        "\n",
        "# Convert data to H2OFrame\n",
        "train_h2o = h2o.H2OFrame(train)\n",
        "test_h2o = h2o.H2OFrame(test)\n",
        "\n",
        "# Train AutoML\n",
        "aml = H2OAutoML(max_models=20, seed=1, sort_metric=\"R2\")\n",
        "aml.train(x=[\"f1\", \"f2\", \"f3\", \"f4\", \"f5\", \"f6\"], y=\"target\", training_frame=train_h2o)\n",
        "\n",
        "# Leaderboard\n",
        "lb = aml.leaderboard\n",
        "print(lb)\n",
        "\n",
        "# Best model\n",
        "best_model = aml.leader\n",
        "predictions = best_model.predict(test_h2o)\n",
        "\n",
        "# Save predictions\n",
        "test[\"target\"] = h2o.as_list(predictions, use_pandas=True)\n",
        "\n",
        "\n",
        "# Shutdown H2O\n",
        "h2o.shutdown(prompt=False)\n"
      ],
      "metadata": {
        "id": "2mebVbyJdZqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MkKQuiW6db-J"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}